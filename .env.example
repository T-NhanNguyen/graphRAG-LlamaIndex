# ==============================================================================
# GraphRAG Llama Index - Environment Configuration Template
# ==============================================================================
# Instructions:
# 1. Copy this file to .env: `cp .env.example .env`
# 2. Update the values below to match your local setup.
# ==============================================================================

# --- [RECOMMENDED LOCAL SETUP] ---
# These settings match the Docker Model Runner configuration used in the guide.

# Embedding Configuration
EMBEDDING_PROVIDER=docker_model_runner
EMBEDDING_MODEL=ai/qwen3-embedding:latest
EMBEDDING_URL=http://host.docker.internal:12434
EMBEDDING_DIMENSION=2560   # Use 2560 for Qwen3, 1024 for Granite

# LLM Configuration (Entity Extraction)
LLM_URL=http://host.docker.internal:12434/v1
LLM_MODEL=ai/granite-4.0-micro:latest
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2048

# Extraction Mode
# Options: local_llm (LLM Only), gliner_llm (GLiNER + LLM Hybrid)
ENTITY_EXTRACTION_MODE=local_llm

# --- [OPTIONAL: OPENROUTER / CLOUD] ---
# Enable this to offload complex relationship reasoning to the cloud.
RELATIONSHIP_PROVIDER=local
OPENROUTER_API_KEY=sk-or-v1-your-api-key-here
OPENROUTER_MODEL=xiaomi/mimo-v2-flash:free

# --- [STORAGE & PATHS] ---
DUCKDB_PATH=./.DuckDB/graphrag.duckdb
INPUT_DIR=./input
OUTPUT_DIR=./output

# --- [RETRIEVAL TUNING] ---
FUSION_ALPHA=0.5
TOP_K=10
RRF_K=60
